library(tidyverse)
library(tidymodels)
library(HMRCstyles)
library(probably)
library(tidyposterior)
library(rstanarm)
tidymodels_prefer()

rs_get_threshold_perf <- function(resamples_df, best_model,  thresholds = seq(0.5, 1, by = 0.0025)){
  # -------------------
  # Generate performance metrics across probability thresholds for resamples.
  # 
  # Description
  #
  # This function takes a dataframe generated by a tidymodels resampling function (e.g. fit_resamples()) and generates performance metrics for each resample using probably::threshold_perf()
  # 
  # Arguments
  # 
  # resamples_df: a dataframe or tibble; the result of a tidymodels resampling function. It assumes one row per observation.
  # best_model: chracter string; the name of the best model, which is required when using tuning. It can be accuired using tune::select_best()
  # thresholds; a numeric vector; the probabilty thresholds on which to evaluate performance.
  # 
  # Value
  # A dataframe or tibble; resamples_df with an additional column .threshold_perf.
  # -------------------
  resamples_df$.predictions <- map(resamples_df$.predictions, 
                                              function(x) x %>% filter(.config == as.character(best_model$.config)))
  resamples_df$.metrics <- map(resamples_df$.metrics, 
                                   function(x) x %>% filter(.config == as.character(best_model$.config)))
  thres_truth <- attributes(resamples_df)$outcomes
  thres_estimate <- ".pred" %>% paste(levels(pull(resamples_df$.predictions[[1]][thres_truth]))[1], sep = "_")
  assess_j_index <- function(single_fold_predictions, thresholds, thres_truth, thres_estimate){
    threshold_data <- single_fold_predictions %>%
      threshold_perf(all_of(thres_truth), all_of(thres_estimate), thresholds = thresholds) %>%
      filter(.metric != "distance") %>%
      mutate(group = case_when(
        .metric == "sens" | .metric == "spec" ~ "1",
        TRUE ~ "2")) %>%
      mutate(.config = best_model$.config) 
    return(threshold_data) 
  }
  resamples_thresh_perf_df <- resamples_df
  resamples_thresh_perf_df$.threshold_perf <- resamples_df$.predictions %>%
    map(assess_j_index, thresholds = thresholds, 
        thres_truth = thres_truth, thres_estimate = thres_estimate)
  return(resamples_thresh_perf_df)
}


rs_get_max_j_index_thres <- function(resamples_thresh_perf_df){
  # -------------------
  # Find the threshold(s) at which the j_index is maximised for each resample.
  # 
  # Description
  #
  # This function takes a dataframe generated by rs_get_threshold_perf() and finds the threshold(s) at which the j_index is maximised for each resample.
  # 
  # Arguments
  # 
  # resamples_thresh_perf_df: a dataframe or tibble; the result of rs_get_threshold_perf()
  #
  # Value
  # A dataframe or tibble; resamples_thresh_perf_df with an additional column .max_j_index_threshold.
  # -------------------
  find_max <- function(single_fold_threshold){
    max_j_index_threshold <- single_fold_threshold %>%
      filter(.metric == "j_index") %>%
      filter(.estimate == max(.estimate)) %>%
      pull(.threshold)
    
  }
  resamples_max_j_df <- resamples_thresh_perf_df
  resamples_max_j_df$.max_j_index_threshold <- resamples_thresh_perf_df$.threshold_perf %>%
    map(find_max)
  return(resamples_max_j_df)
}

rs_plot_thres_vs_metrics <- function(resamples_max_j_df, x_label = NULL){
  # -------------------
  # Plot probability thresholds vs metrics for each resample.
  # 
  # Description
  #
  # This function plots for each resample probaility thresholds on the x-axis and estimates of the j_index, specificity and sensitiivity on the y-axis, as well as vertical lines representing the thresholds where the maximum j_index values were estimated. 
  # 
  # Arguments
  # 
  # resamples_max_j_df: a dataframe or tibble; the result of rs_get_max_j_index_thres().
  # x_label: character string; the x-axis lebel. If NULL (default), then it reverts to "<truth> threshold\n( above this value, prediction is considered <truth )", where <truth> is the category that the model pedicts. 
  #
  # Value
  # A dataframe or tibble; resamples_max_j_df is returned as a side effect so that the this function can be chained.
  # -------------------
  .outcome <- attributes(resamples_max_j_df)$outcomes
  .truth <- levels(pull(resamples_max_j_df$.predictions[[1]][.outcome]))[1]
  if (is.null(x_label)){
    x_label <- paste(.truth, "threshold\n( above this value, prediction is considered", .truth, ")", sep = " ")
  }
  col1 <- as.numeric(str_extract(resamples_max_j_df$id, "[0-9]+")) %>% rep(map_dbl(resamples_max_j_df$.max_j_index_threshold, length))
  col2 <- resamples_max_j_df$.max_j_index_threshold %>% unlist()
  vline_df <- tibble(fold = col1, .max_j_index_thres_multi = col2)
  pt <- bind_rows(resamples_max_j_df$.threshold_perf, .id = "fold") %>%
    ggplot(aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line()  +
    geom_vline(data  = vline_df, 
               aes(xintercept = .max_j_index_thres_multi), color = "grey30") +
    scale_colour_HMRC_d() +
    theme_classic_HMRC() +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    labs(
      x = x_label,
      y = "Metric Estimate"
    ) +
    facet_wrap(~ fold, scales = "free", ncol = 2)
  print(pt)
  invisible(resamples_max_j_df)
}

rs_optimise_predictions <- function(resamples_max_j_df, multi_metric = metric_set(accuracy, sens, yardstick::spec, ppv, j_index), multi_threshold_fun = mean){
  # -------------------
  # Optimise predictions based on thresholds with maximum j_index values for each resample.
  # 
  # Description
  #
  # This function takes a dataframe generated by rs_get_max_j_index_thres() and recaclulates model predictions based on thresholds with maximum j_index values for each resample. Where for a resample, multiple thresholds yielded the same maximum j_index value, a summary threshold is used. 
  # 
  # Arguments
  # 
  # resamples_max_j_df: a dataframe or tibble; the result of rs_get_max_j_index_thres()
  # multi_threshold_fun: a function; the summary function used to calculate a summary threshold (e.g. mean, median, min, max)
  #
  # Value
  # A dataframe or tibble; resamples_max_j_df with an additional column .optimised_predictions.
  # -------------------
  thres_truth <- attributes(resamples_max_j_df)$outcomes
  thres_estimate <- ".pred" %>% paste(levels(pull(resamples_max_j_df$.predictions[[1]][thres_truth]))[1], sep = "_")
  recode_pred <- function(single_fold_predictions, single_fold_max_j_thres, thres_truth, thres_estimate, best_model){
    recoded_pred <- single_fold_predictions %>%
      mutate(
        .pred = make_two_class_pred(
          estimate = pull(single_fold_predictions[thres_estimate]), 
          levels = levels(pull(single_fold_predictions[thres_truth])), 
          threshold = multi_threshold_fun(single_fold_max_j_thres)
        )
      ) %>% mutate(.threshold = multi_threshold_fun(single_fold_max_j_thres))
    return(recoded_pred)
  }
  resamples_optimised <- resamples_max_j_df
  resamples_optimised$.predictions <- map2(resamples_max_j_df$.predictions, resamples_max_j_df$.max_j_index_threshold, 
                                                     recode_pred, thres_truth = thres_truth, 
                                                     thres_estimate = thres_estimate)
  metrics_multi <- metric_set(j_index, ppv, yardstick::spec, sens)
  
  
  resamples_optimised$.metrics <- resamples_optimised$.predictions %>%
    map2(resamples_optimised$.metrics, function(x, y) multi_metric(x, truth = pull(x[thres_truth]), estimate = .pred) %>%
          mutate(.config = unique(y$.config)))
  
  resamples_optimised <- resamples_optimised %>% select(-.threshold_perf, -.max_j_index_threshold)
  attr(resamples_optimised, "workflow") <- attributes(resamples_max_j_df)$workflow
  attributes(resamples_optimised)$parameters <- NULL
  return(resamples_optimised)
}

rs_summarise_optimum_threshold <- function(collected_optimised_predictions, summary_fun = mean){
  # -------------------
  # Get a summary of the optimum thresholds across resamples.
  # 
  # Arguments
  # 
  # collected_optimised_predictions: a dataframe or tibble; a dataframe or tibble; the result of collect_predictions() on optmised resamples.
  # summary_fun: a function; the summary function used to calculate the summary of optimum threshold (e.g. mean, median, min, max) across resamples
  #
  # Value
  # A signle number
  # -------------------
  collected_optimised_predictions %>%
  distinct(.threshold) %>% 
    pull() %>% 
    summary_fun()
}

# load an example dataset
data(cells, package = "modeldata")
cells <- cells[, 2:11] %>% arrange(class) %>% slice(1:1500)

# split into training and testing datasets and create folds for cross-validation
set.seed(26)
cell_split <- initial_split(cells, 
                            strata = class)

cell_train <- training(cell_split)
cell_test  <- testing(cell_split)

folds <- vfold_cv(cell_train, v = 5)

# train a random forest model with resampling
rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_wf <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(class ~ .)

multi_metric <- metric_set(accuracy, sens, yardstick::spec, ppv,  mn_log_loss, roc_auc, j_index)

rf_fit_rs <- 
  rf_wf %>% 
  fit_resamples(folds, control = control_resamples(save_pred = TRUE, save_workflow = TRUE), metrics = multi_metric)  

rf_best <- select_best(rf_fit_rs,
                             metric = "mn_log_loss")

# find the probability thresholds that have the best trade-offs between specificity and sensitivity 
rf_opt <- rf_fit_rs %>% rs_get_threshold_perf(best_model = rf_best) %>% rs_get_max_j_index_thres() %>% rs_plot_thres_vs_metrics() %>% rs_optimise_predictions()

# Visualise predictions in a histogram
rf_opt_pred <- rf_opt %>% collect_predictions()

rf_opt_pred %>%
  ggplot() +
  geom_histogram(aes(x = .pred_PS, fill = class), alpha = 0.7) +
  geom_vline(aes(xintercept = rs_summarise_optimum_threshold(rf_opt_pred)), alpha = .6, color = "grey30") +
  scale_fill_HMRC_d() +
  theme_classic_HMRC() +
  labs(
    x = "'PS' Threshold\n(above this value is considered 'PS')"
  )

# train a penalised logistic regression model with resampling and tuning
pen_lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>%
  set_mode("classification")

pen_lr_recipe <- recipe(class ~ ., data = cell_train) %>% 
  step_dummy(all_nominal_predictors())

pen_lr_wf <- 
  workflow() %>% 
  add_model(pen_lr_mod) %>% 
  add_recipe(pen_lr_recipe)

pen_lr_fit_rs <- 
  pen_lr_wf %>%
  tune_grid(resamples = folds,
            grid =  grid_regular(parameters(pen_lr_wf), levels = 30),
            control = control_grid(save_pred = TRUE, save_workflow = TRUE),
            metrics = multi_metric)

pen_lr_best <- select_best(pen_lr_fit_rs,
                           metric = "mn_log_loss")

# find the probability thresholds that have the best trade-offs between specificity and sensitivity
pen_lr_opt <- pen_lr_fit_rs %>% rs_get_threshold_perf(best_model = pen_lr_best) %>% rs_get_max_j_index_thres() %>% rs_plot_thres_vs_metrics() %>% rs_optimise_predictions()

# Visualise predictions in a histogram
pen_lr_opt_pred <- pen_lr_opt %>% collect_predictions()

pen_lr_opt_pred %>%
  ggplot() +
  geom_histogram(aes(x = .pred_PS, fill = class), alpha = 0.7) +
  geom_vline(aes(xintercept = rs_summarise_optimum_threshold(pen_lr_opt_pred)), alpha = .6, color = "grey30") +
  scale_fill_HMRC_d() +
  theme_classic_HMRC() +
  labs(
    x = "'PS' Threshold\n(above this value is considered 'PS')"
  )

# Statistically compare ppv values between models
two_models <- as_workflow_set(rf = rf_opt, pen_lr = pen_lr_opt)

ppv_indiv_estimates <- 
  collect_metrics(two_models, summarize = FALSE) %>% 
  filter(.metric == "ppv")

ppv_wider <- 
  ppv_indiv_estimates %>% 
  select(wflow_id, .estimate, id) %>% 
  pivot_wider(id_cols = "id", names_from = "wflow_id", values_from = ".estimate")

# compare ppv using a linear model
compare_lm <- 
  ppv_wider %>% 
  mutate(difference = rf - pen_lr)

lm(difference ~ 1, data = compare_lm) %>% 
  tidy(conf.int = TRUE) %>% 
  select(estimate, p.value, starts_with("conf"))

# compare ppv using a paired t-test
ppv_wider %>% 
  with( t.test(rf, pen_lr, paired = TRUE) ) %>%
  tidy() %>% 
  select(estimate, p.value, starts_with("conf"))

# compare ppv using bayesain anova 
ppv_anova <-
  perf_mod(
    two_models,
    metric = "ppv",
    prior_intercept = rstanarm::student_t(df = 1),
    chains = 4,
    iter = 5000,
    seed= 1102
  )

model_post <- 
  ppv_anova %>% 
  # Take a random sample from the posterior distribution
  # so set the seed again to be reproducible. 
  tidy(seed = 1103)

model_post %>% 
  mutate(model = forcats::fct_inorder(model)) %>%
  ggplot(aes(x = posterior)) + 
  geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) + 
  facet_wrap(~ model, ncol = 1)
